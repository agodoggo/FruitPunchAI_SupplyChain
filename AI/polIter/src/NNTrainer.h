#pragma once
#include "SampleData.h"
#include "HardwareUtilities.h"
#include <string>
#include <fstream>
#include "MDPAdapter.h"
#include "Settings.h"

#ifdef __linux__
//In my case, Torch is only available on the linux server. 
//There is a torch for visual studio c++, but it is not stable (July 2019)
//so not recommended to try to get that one to work at present..:q
#define TorchAvailable true
#endif
#if TorchAvailable
#include <torch/torch.h>
#include "NNModels.h"
#include "RNG.h"
#endif

template <class MDP>
class NNTrainer
{
	RNG rng{ 11112014 };
	MDPAdapter<MDP> adapter;
public:
	
	struct TrainSettings
	{
		TrainSettings(size_t SamplesUsed, size_t MiniBatchSize, std::vector<int64_t> hiddenLayers, size_t maxEpochs,double AdamParam,size_t version):
			SamplesUsed{ SamplesUsed }, MiniBatchSize{ MiniBatchSize }, hiddenLayers{ hiddenLayers }, maxEpochs{ maxEpochs }, AdamParam{ AdamParam }, version{ version }
		{
			if (hiddenLayers.size() > 5)
			{
				std::cout << "more than 5 hidden layers currently not supported " << std::endl;
				throw "not supported";
			}
		}
		size_t SamplesUsed;
		size_t MiniBatchSize;
		std::vector<int64_t> hiddenLayers;
		size_t maxEpochs;
		double AdamParam;
		size_t version;

		float InSampleLoss{ 0.0f };
		float OutOfSampleLoss{ 0.0f };
		float cost{ 0.0f };
		size_t actualepochs{ 0 };


		double time{ 0.0 };
		void print()
		{
			std::cout << "samples: " << SamplesUsed << " mBatchSize: " << MiniBatchSize << " hidden: (";
			
			for (size_t i = 0; i < hiddenLayers.size(); i++)
			{
				std::cout << hiddenLayers[i] << " ";
			}

			std::cout << ")  actual/max epochs: " << actualepochs << "/" << maxEpochs << " Loss(in s.: " << InSampleLoss << " )(out of s.: " << OutOfSampleLoss << " )  Cost imp: " << cost << " time " << time << std::endl;
		}
	};

	size_t agentGeneration;

	
	
	NNTrainer(const MDP* mdp_ptr, size_t agentGeneration)
		:adapter{ mdp_ptr }, agentGeneration{ agentGeneration }, trainingData{}
	{
		if (agentGeneration == 0)
		{
			std::cout << "cannot train zeroth generation agent" << std::endl;
			throw "cannot train zeroth generation agent";
		}
		//We train an agent based on data generated by the previous generation agent..
		trainingData.Load(adapter.Identifier(), agentGeneration - 1);
		std::cout << "Training generation " << agentGeneration << " using "; trainingData.PrintStatistics();
	}


	SampleData<MDP> trainingData;

	void TrainEnsemble()
	{
#ifndef TorchAvailable
		std::cout << "torch not available, cannot train";
		throw "torch not available, cannot train";
#endif
		//Version=0 is reserved for selected version. 
		size_t version = 1;
		std::vector<TrainSettings> settingsPack;

		size_t samplesused{ 0 };

		for (size_t samplesused_max : {Settings::Current.MaxSamples})
		{
			samplesused = std::min(samplesused_max, trainingData.samples.size());
			for (auto hiddenLayers : std::vector<std::vector<int64_t>>{/* {150,150,150} ,*/ /*,{180,130,100,100,100}*/ Settings::Current.HiddenLayers /*, {180,130,100,80},{300,50,50},{200,100,50,50},{150,100,50,50,50}*/ })
			{
				for (size_t MiniBatchSize : {/*32,*//*96,*/Settings::Current.MiniBatchSize/*128,64,256*/})
				{
					size_t MaxEpochs{ 2500 };//2500
					TrainSettings settings(samplesused, MiniBatchSize, hiddenLayers, MaxEpochs, /*adamParam*/ 0.5, version++);
					settingsPack.push_back(settings);
				}
			}
		}
		for (auto& settings : settingsPack)
		{
			createBatches(settings.MiniBatchSize, settings.SamplesUsed);
			Train(settings);
		}
		
		double bestLoss = std::numeric_limits<double>::max();
		size_t IndexOfBestSettings = 0;
		for (size_t i=0;i< settingsPack.size();i++)
		{
			TrainSettings& settings = settingsPack[i];
			if (settings.OutOfSampleLoss< bestLoss)
			{
				bestLoss = settings.OutOfSampleLoss;
				IndexOfBestSettings = i;
			}
			settings.print();		    
		}
		SelectVersion(settingsPack[IndexOfBestSettings]);
	}

	void SelectVersion(TrainSettings& settings)
	{
#if TorchAvailable


	

		LayerModel model{};
		model->LoadMetaInfo(HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration , settings.version, true));
		std::string loc = HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration , settings.version,false);
		torch::load(model, loc);
		torch::Device cpudevice(torch::kCPU);
		model->to(cpudevice);
		size_t IndexOfDefaultBestVersion = 0;
		std::string saveloc = HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration ,IndexOfDefaultBestVersion,false);
		torch::save(model, saveloc);
		model->SaveMetaInfo(HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration, IndexOfDefaultBestVersion, true));

#endif
	}
private:
	void Train(TrainSettings& settings)
	{
		auto time = std::chrono::steady_clock::now();	
#if TorchAvailable		
		
		torch::Device device(torch::kCPU);
		if (torch::cuda::is_available())
		{
			std::cout << "cuda available " << std::endl;
			device = torch::Device(torch::kCUDA, 0);
		}
		else
		{
			std::cout << "cuda not available " << std::endl;
		}
		size_t NumFeats = adapter.NumFeatures();
		size_t NumberOfActions = adapter.TotalValidActions();

		LayerModel model = LayerModel{ NumFeats,NumberOfActions,settings.hiddenLayers };
		

    	torch::optim::Adam opt(model->parameters(), torch::optim::AdamOptions(2e-4).beta1(settings.AdamParam).weight_decay(0.0));
		

		auto timemodel2 = std::chrono::steady_clock::now();
		model->to(device);
		std::cout << "model and optimizer created: " << ((timemodel2 - time).count() / 1000000) << std::endl;
		
	
		float BestCosts = std::numeric_limits<float>::infinity();
		float Best_OOS_Loss = std::numeric_limits<float>::infinity();
		float BestInSampleLoss = std::numeric_limits<float>::infinity();
		size_t epoch = 0;
		size_t lastImprovedEpoch = 0;
		do
		{
			{
			//	auto time3 = std::chrono::steady_clock::now();
				createBatches(settings.MiniBatchSize, settings.SamplesUsed);
			//	auto time4 = std::chrono::steady_clock::now();
			//	std::cout << "Created randomized batches: " << ((time4 - time3).count() / 1000000) << " ms " << std::endl;
			}



			float loss_in_sample = 0.0;
			for (MiniBatch& minibatch : TrainingBatches)
			{
				opt.zero_grad();
				auto forward = model->forward(minibatch.features) - minibatch.mask;
				torch::Tensor loss = torch::nll_loss(torch::log_softmax(forward, 1), minibatch.labels);
				loss.backward();
				opt.step();
				loss_in_sample += loss.item<float>();
			}
			loss_in_sample /= TrainingBatches.size();
			BestInSampleLoss = std::min(loss_in_sample, BestInSampleLoss);
			if (epoch % 5 == 0)
			{
				torch::NoGradGuard guard;
				auto forward = model->forward(testBatch.features) - testBatch.mask;
				auto logsoftMax = torch::log_softmax(forward, 1);
				torch::Tensor loss = torch::nll_loss(logsoftMax, testBatch.labels);
				auto probs = torch::exp(logsoftMax);
				auto costsPerSide = probs * testBatch.costs;
				//Does not work well with auto instead of torch::Tensor. 
				//Somehow it casts this to other type. 
				torch::Tensor costs = torch::sum(costsPerSide);
				auto costsNorm = costs.item<float>() / testBatch.BatchSize;
				BestCosts = std::min(BestCosts, costsNorm);
		

				auto time2 = std::chrono::steady_clock::now();

				float loss_OOS = loss.item<float>();
		
				if (loss_OOS > Best_OOS_Loss)
				{
					if (epoch - lastImprovedEpoch > 15)
					{
						break;
					}
					std::cout << "   ";
				}
				else
				{
					//Negligible overhead, about 10 ms for multilayer perceptron. 
					torch::save(model, HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration, settings.version, false));
					model->SaveMetaInfo(HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration, settings.version, true));
					std::cout << "S: ";

					lastImprovedEpoch = epoch;
				}
				Best_OOS_Loss = std::min(loss_OOS, Best_OOS_Loss);
				std::cout << "EPOCHS: " << epoch << " - loss (OOS): " << loss_OOS << " -> " << (int)(100*std::exp(-loss_OOS)) << "% (IS): " << loss_in_sample << " -> " << (int)(100 * std::exp(-loss_in_sample)) << "% Costs " << costsNorm << " Time: " << ((time2 - time).count() / 1000000000) << std::endl;

			}
			epoch++;
		} while (epoch < settings.maxEpochs);
		settings.actualepochs = epoch;
		settings.InSampleLoss = BestInSampleLoss;
		settings.OutOfSampleLoss = Best_OOS_Loss;
		settings.cost = BestCosts;
		auto timetotal = std::chrono::steady_clock::now();
		settings.time = ((timetotal - time).count() / 1000000000);
		std::cout << "best loss " << Best_OOS_Loss << " Best Costs " << BestCosts << std::endl;


		torch::load(model, HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration, settings.version, false));
		torch::Device cpudevice(torch::kCPU);
		model->to(cpudevice);
		torch::save(model, HardwareUtilities::PathToModelFile(adapter.Identifier(), agentGeneration ,settings.version,false));

#endif
	}


#if TorchAvailable

	
	struct MiniBatch
	{
		MiniBatch()
		{
		
		}
		MiniBatch(long BatchSize, long numFeats, long numLabels/*,torch::Device device*/) :BatchSize{ BatchSize }
		{
			auto options =
				torch::TensorOptions()
				.dtype(torch::kInt64);
			features = torch::empty({ BatchSize,numFeats });
			mask = torch::full({ BatchSize,numLabels }, 32);
			costs = torch::full({ BatchSize,numLabels }, 32);
			labels = torch::empty({ BatchSize }, options);
		}
		void to(torch::Device & device, bool costAlso = false)
		{
			features = features.to(device);
			mask = mask.to(device);
			labels = labels.to(device);
			if (costAlso)
			{
				costs = costs.to(device);
			}
		}
		long BatchSize;
		torch::Tensor features;
		torch::Tensor mask;
		torch::Tensor labels;
		torch::Tensor costs;
	};


	MiniBatch CreateBatch(SampleData<MDP>& dataSource, size_t Point, size_t BatchSize)
	{

		//Add the features
		auto NumFeats = adapter.NumFeatures();
		auto TotalActions = adapter.TotalValidActions();

		MiniBatch batch((long)(BatchSize), (long)(NumFeats), (long)(TotalActions));


		torch::Tensor& feats = batch.features;
		float* batch_feats_ptr = feats.data<float>();
		torch::Tensor& labels = batch.labels;
		long* batch_label_ptr = labels.data<long>();
		torch::Tensor& mask = batch.mask;
		float* batch_mask_ptr = mask.data<float>();
		torch::Tensor& costs = batch.costs;
		float* batch_cost_ptr = costs.data<float>();

	



		for (size_t item = 0; item < BatchSize; item++)
		{
			//Select the point corresponding to this
			auto& sample = dataSource.samples[Point++];


			size_t offset = sample.offSet;
			
			std::vector<size_t>* allowedActions;

			std::vector<size_t> actionsExtractedFromState;

			if constexpr (Sample<MDP>::HasState)
			{//Extract features live
				offset = adapter.ActionOffset(sample.state);
				actionsExtractedFromState = adapter.GetAllowedActions(sample.state);
				allowedActions = &actionsExtractedFromState;
				//Quickly extract features:
				adapter.ExtractFeatures(sample.state, (batch_feats_ptr + item * NumFeats));
			}
			else
			{//Used stored features
				allowedActions = &sample.allowedActionsInState;
				for (size_t i = 0; i < NumFeats; i++)
				{
					*(batch_feats_ptr + item * NumFeats + i) = sample.features[i];
				}
			}

			//Add the correct label
			batch_label_ptr[item] = (long)(sample.BestAction + offset);



			int actionIter = 0;
			//Remove the mask for those actions that are legal.
			//Add costs for those actions that are legal.
			for (auto action : *allowedActions)
			{
				size_t TransformedAction = action + offset;
				if (TransformedAction > TotalActions)
				{
					throw "Action larger than allowed actions";
				}
				*(batch_mask_ptr + item * TotalActions + TransformedAction) = 0.0;
				*(batch_cost_ptr + item * TotalActions + TransformedAction) = sample.ValPerAllowedAction[actionIter++];
			}
		}
		return batch;
	}

	MiniBatch testBatch;
	std::vector<MiniBatch> TrainingBatches;

	


#endif
	void createBatches(size_t miniBatchSize, size_t maxSamplesUsed = std::numeric_limits<size_t>::max(), bool forceCPU = false)
	{
#if TorchAvailable

		torch::Device device(torch::kCPU);
		if (torch::cuda::is_available()&&!forceCPU)
		{
			device = torch::Device(torch::kCUDA, 0);
		}
		//auto time = std::chrono::steady_clock::now();

		size_t NumDataTotal = std::min(trainingData.samples.size(), maxSamplesUsed);
		size_t NumTrain = (size_t)(NumDataTotal * 0.95);
		
		size_t numMiniBatches = NumTrain / miniBatchSize;
		if (numMiniBatches == 0)
		{
			numMiniBatches = 1;
			miniBatchSize = NumTrain;
		}
		TrainingBatches.clear();
		TrainingBatches.reserve(numMiniBatches);

		size_t samplesInTest = miniBatchSize * numMiniBatches;


		//Shuffle the examples.. 
		std::shuffle(trainingData.samples.begin(), trainingData.samples.begin()+samplesInTest, rng.gen_);

		size_t samplesUsed = 0;

		for (size_t miniBatchIter = 0; miniBatchIter < numMiniBatches; miniBatchIter++)
		{			
			TrainingBatches.push_back(CreateBatch(trainingData, samplesUsed, miniBatchSize));
			TrainingBatches.back().to(device, false);
			samplesUsed += miniBatchSize;
		}


		size_t NumTest = NumDataTotal - samplesUsed;
		testBatch = CreateBatch(trainingData, samplesUsed, NumTest);
		testBatch.to(device, true);
	//	auto timeB = std::chrono::steady_clock::now();
	//	std::cout << "MINIBATCH SIZE: " <<  miniBatchSize <<  "; batches created, timing: " << ((timeB - time).count() / 1000000000) << std::endl;
#endif
	}

};


